## MEmoR: An Intelligent Framework for Multimodal Emotion Recognition
Multimodal emotion recognition (MER) involves detecting
and understanding human emotions by analyzing multiple modalities,
such as images, audio, videos, and texts. MER is a challenging problem
due to the complexities of multiple modalities and fusing their information to interpret and classify human emotions accurately. This paper
introduces an intelligent framework (MEmoR) for multimodal emotion
recognition. It focuses on the challenging domain of emotion detection
within a Bengali audio-visual dataset. A vital aspect of this work involves creating a new dataset, a multimodal emotion recognition dataset
(MERD), tailored to specific task requirements. The MERD encompasses 1937 annotated multimodal data across four categories: happy,
sad, angry, and neutral. The proposed framework utilizes various machine learning (ML), deep learning (DL), and transformer-based models
for audio and visual modalities. This work explores and integrates audio
and visual modalities through feature-level and decision-level fusion.


**Author**: Md. Tanvir Rahman, Shawly Ahsan, Jawad Hossain, Mohammed3 Moshiul Hoque, and M. Ali Akber Dewan

## Dataset

Unlock the power of multimodal emotion recognition with our meticulously curated dataset. Dive into the rich, diverse, and annotated Multimodal Emotion Recognition Dataset (MERD), designed to elevate your research and projects.



Access the dataset through the following link:

[Dataset](https://drive.google.com/drive/folders/1zJK4jNqvHHkEV392CHeaIDnPVmWHQVWW?usp=sharing)